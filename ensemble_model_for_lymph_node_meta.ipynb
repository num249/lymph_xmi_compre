{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbef4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, cross_val_predict\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.neighbors import KNeighborsClassifier   \n",
    "from sklearn import svm  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "import datetime\n",
    "import multiprocessing as mp\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "# cores = mp.cpu_count()\n",
    "# pool = mp.Pool(processes=20)\n",
    "\n",
    "# min1=min(list(X_standard.shape))\n",
    "# a1=list(range(10,min1,10))\n",
    "# a1[-1]=min1\n",
    "# pcc_top1=pool.map(find_best_pca_lr,a1)  ##（SVM，LR）\n",
    "# pool.terminate()\n",
    "\n",
    "\n",
    "# a1=pd.DataFrame()\n",
    "# for i1 in pcc_top1:\n",
    "#     a1=pd.concat([a1,i1], axis=0)\n",
    "# a1.columns=['tr_TP', 'tr_TN', 'tr_FP', 'tr_FN', 'tr_Sen', 'tr_Spe', 'tr_Acc', 'tr_PPV', 'tr_NPV', 'tr_MCC', 'tr_AUC','TP',\n",
    "#        'TN', 'FP', 'FN', 'Sen', 'Spe', 'Acc', 'PPV', 'NPV', 'MCC', 'AUC','PCA']   \n",
    "\n",
    "# a1=a1.sort_values(['Acc','tr_Acc','PCA'], ascending=[False,False,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a00ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_csv(input_path,i):\n",
    "\n",
    "    df_chunk=pd.read_csv(input_path,\n",
    "                         chunksize=10000,\n",
    "#                          sep='\\t'\n",
    "                        sep=i\n",
    "                        )\n",
    "    res_chunk=[]\n",
    "    for chunk in df_chunk:\n",
    "        res_chunk.append(chunk)\n",
    "    res_df=pd.concat(res_chunk)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d3ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measures_gridloo(label, score):\n",
    "    label = np.array(label)\n",
    "    score = np.array(score)\n",
    "    \n",
    "    N  = len(label)\n",
    "    TP = sum((label == 1) & (score == 1))\n",
    "    TN = sum((label == 0) & (score == 0))\n",
    "    FP = sum((label == 0) & (score == 1))\n",
    "    FN = sum((label == 1) & (score == 0))\n",
    "\n",
    "    # init all measures to nan\n",
    "    measures = {measure: float(\"nan\") for measure in (\"Sen\", \"Spe\", \"Acc\", \"PPV\", \"NPV\", \"MCC\",\"AUC\")}\n",
    "    \n",
    "    measures[\"TP\"] = TP\n",
    "    measures[\"TN\"] = TN\n",
    "    measures[\"FP\"] = FP\n",
    "    measures[\"FN\"] = FN\n",
    "    \n",
    "    S = (TP + FN) / N\n",
    "    P = (TP + FP) / N\n",
    "\n",
    "    if (TP + FN) > 0: #recall\n",
    "        measures[\"Sen\"] = round(TP/(TP+FN), 4)\n",
    "\n",
    "    if (TN + FP) > 0:\n",
    "        measures[\"Spe\"] = round(TN/(TN+FP), 4)\n",
    "\n",
    "    if (TP + FP + FN + TN) > 0:\n",
    "        measures[\"Acc\"] = round((TP+TN)/(TP+FP+FN+TN), 4)\n",
    "\n",
    "    if (TP + FP) > 0: #precision\n",
    "        measures[\"PPV\"] = round(TP/(TP+FP), 4)\n",
    "\n",
    "    if (TN + FN) > 0:\n",
    "        measures[\"NPV\"] = round(TN/(TN+FN), 4)\n",
    "\n",
    "    if (S*P*(1-S)*(1-P)) > 0:\n",
    "        measures[\"MCC\"] = round((TP/N - S*P)/(math.sqrt(S*P*(1-S)*(1-P))), 4)\n",
    "    \n",
    "    \n",
    "    measures[\"AUC\"]= roc_auc_score(label, score)\n",
    "    return pd.DataFrame([measures],\n",
    "                        columns=[\"TP\", \"TN\", \"FP\",\n",
    "                                 \"FN\", \"Sen\", \"Spe\", \"Acc\", \"PPV\", \"NPV\", \"MCC\",\"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d53b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SVM\n",
    "def find_best_pca(i):\n",
    "    n_components=i\n",
    "    estimator = PCA(n_components=n_components,random_state=10)\n",
    "    pca_X_train = estimator.fit_transform(X_standard)\n",
    "    pca_X_test = estimator.transform(X_standard_test)\n",
    "    \n",
    "    scoring = {'AUC': 'roc_auc'}\n",
    "    cost = [-5, -3, -1, 1, 3, 5, 7, 9, 11, 13, 15]\n",
    "    gam = [3, 1, -1, -3, -5, -7, -9, -11, -13, -15]\n",
    "    parameters =[{'kernel': ['rbf'], 'C': [2**x for x in cost],'gamma':[2**x for x in gam]}]\n",
    "    \n",
    "    cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=10) \n",
    "    grid_search=GridSearchCV(estimator=SVC(random_state=10),param_grid=parameters,cv=cvx,scoring='accuracy')\n",
    "    grid_search.fit(pca_X_train, train_y)\n",
    "    \n",
    "    train_pre_y = cross_val_predict(grid_search.best_estimator_, pca_X_train,train_y, cv=cvx)\n",
    "    train_res1=get_measures_gridloo(train_y,train_pre_y)\n",
    "#     train_res1['PCA']=n_components\n",
    "    test_pre_y = grid_search.predict(pca_X_test)\n",
    "    test_res1=get_measures_gridloo(test_y,test_pre_y)\n",
    "    test_res1['PCA']=n_components\n",
    "    \n",
    "    res=pd.concat([train_res1,test_res1], axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4e5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_pca_lr(i):\n",
    "    n_components=i\n",
    "    estimator = PCA(n_components=n_components,random_state=10)\n",
    "    pca_X_train = estimator.fit_transform(X_standard)\n",
    "    pca_X_test = estimator.transform(X_standard_test)\n",
    "    \n",
    "    cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=10) \n",
    "\n",
    "    param_grid = {'penalty':['l2'],\n",
    "              \"C\":[0.00001,0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000],  \n",
    "              \"solver\":[\"newton-cg\", \"lbfgs\",\"liblinear\"],\n",
    "#             \"C\":list(np.arange(0,0.1,0.005)),   \n",
    "#               \"solver\":[\"newton-cg\", \"lbfgs\",\"liblinear\"]\n",
    "            }\n",
    "    LR_grid = LogisticRegression()\n",
    "    grid_search = GridSearchCV(LR_grid, param_grid=param_grid, cv=cvx ,scoring='accuracy',n_jobs=9)\n",
    "    grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "\n",
    "    train_pre_y = cross_val_predict(grid_search.best_estimator_, pca_X_train,train_y, cv=cvx)\n",
    "    train_res1=get_measures_gridloo(train_y,train_pre_y)\n",
    "#     train_res1['PCA']=n_components\n",
    "   \n",
    "\n",
    "    test_pre_y = grid_search.predict(pca_X_test)\n",
    "    test_res1=get_measures_gridloo(test_y,test_pre_y)\n",
    "    test_res1['PCA']=n_components\n",
    "    \n",
    "    res=pd.concat([train_res1,test_res1], axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f27eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer='HNSC'\n",
    "# path='D:\\\\lymph_meta_xmiseq\\\\tcga_data\\\\'\n",
    "path='D:\\\\lymph_meta_xmiseq\\\\tmp\\\\men2\\\\tcga_data\\\\'\n",
    "\n",
    "train=pd.read_csv(path+cancer+\"\\\\admat_final.csv\").iloc[:,2:].T.values\n",
    "target=pd.read_csv(path+cancer+\"\\\\gctm_label.csv\",index_col=0).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4d47e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,test_X, train_y, test_y = train_test_split(train,\n",
    "                                                   target,\n",
    "                                                   test_size = 0.3,\n",
    "                                                   stratify=target,\n",
    "                                                   random_state=10\n",
    "                                                   )\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "standardScaler.fit(train_X)\n",
    "X_standard = standardScaler.transform(train_X)\n",
    "X_standard_test = standardScaler.transform(test_X)\n",
    "\n",
    "n_components=0.95\n",
    "estimator = PCA(n_components=n_components,random_state=10)\n",
    "pca_X_train = estimator.fit_transform(X_standard)\n",
    "pca_X_test = estimator.transform(X_standard_test)\n",
    "cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "385399ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.45230769        nan 0.52369231 0.54769231\n",
      " 0.54769231 0.72153846 0.76123077 0.74523077        nan        nan\n",
      " 0.45230769        nan 0.50030769 0.75292308 0.75292308 0.75292308\n",
      " 0.79230769 0.75292308        nan        nan 0.45230769        nan\n",
      " 0.52369231 0.848      0.848      0.82430769 0.864      0.832\n",
      "        nan        nan 0.68246154        nan 0.75292308 0.888\n",
      " 0.888      0.87230769 0.88       0.88              nan        nan\n",
      " 0.92              nan 0.92       0.88061538 0.88061538 0.85661538\n",
      " 0.86461538 0.86461538        nan        nan 0.944             nan\n",
      " 0.936      0.84061538 0.84061538 0.86461538 0.86461538 0.87261538\n",
      "        nan        nan 0.944             nan 0.87261538 0.84861538\n",
      " 0.84861538 0.86461538 0.86461538 0.87261538        nan        nan\n",
      " 0.90430769        nan 0.87261538 0.85661538 0.85661538 0.84861538\n",
      " 0.86461538 0.87261538        nan        nan 0.80892308        nan\n",
      " 0.87261538 0.84861538 0.84861538 0.84061538 0.86461538 0.87261538]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=10, shuffle=True),\n",
       "             estimator=LogisticRegression(max_iter=1000), n_jobs=10,\n",
       "             param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100,\n",
       "                               1000],\n",
       "                         'penalty': ['l1', 'l2'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#svm\n",
    "scoring = {'AUC': 'roc_auc'}\n",
    "cost = [-5, -3, -1, 1, 3, 5, 7, 9, 11, 13, 15]\n",
    "gam = [3, 1, -1, -3, -5, -7, -9, -11, -13, -15]\n",
    "parameters =[{'kernel': ['rbf'], 'C': [2**x for x in cost],'gamma':[2**x for x in gam]}]\n",
    "cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=10) \n",
    "\n",
    "svc_grid_search=GridSearchCV(estimator=SVC(random_state=10),param_grid=parameters,cv=cvx,scoring='accuracy')\n",
    "svc_grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "#LR\n",
    "param_grid = {'penalty':['l1', 'l2'],\n",
    "              \"C\":[0.00001,0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              \"solver\":[\"newton-cg\", \"lbfgs\",\"liblinear\",\"sag\",\"saga\"]\n",
    "#               \"algorithm\":['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            }\n",
    "LR_grid = LogisticRegression(max_iter=1000)\n",
    "LR_grid_search = GridSearchCV(LR_grid, param_grid=param_grid, cv=cvx ,scoring='accuracy',n_jobs=10)\n",
    "LR_grid_search.fit(pca_X_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c0fc0d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('lr',\n",
       "                                LogisticRegression(C=1, max_iter=1000,\n",
       "                                                   penalty='l1',\n",
       "                                                   solver='liblinear')),\n",
       "                               ('svc',\n",
       "                                SVC(C=32, gamma=3.0517578125e-05,\n",
       "                                    random_state=10))],\n",
       "                   final_estimator=LinearSVC(C=5), n_jobs=10)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('lr', LR_grid_search.best_estimator_),\n",
    "    ('svc', svc_grid_search.best_estimator_),\n",
    "]\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LinearSVC(C=5),n_jobs=10)\n",
    "clf.fit(pca_X_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "aa318404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('lr',\n",
       "                                LogisticRegression(C=1, max_iter=1000,\n",
       "                                                   penalty='l1',\n",
       "                                                   solver='liblinear')),\n",
       "                               ('svc',\n",
       "                                SVC(C=32, gamma=3.0517578125e-05,\n",
       "                                    random_state=10))],\n",
       "                   final_estimator=LogisticRegression(C=100))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('lr', LR_grid_search.best_estimator_),\n",
    "    ('svc', svc_grid_search.best_estimator_),\n",
    "]\n",
    "\n",
    "param_grid = {'final_estimator':[LogisticRegression(C=0.00001),LogisticRegression(C=0.0001),LogisticRegression(C=0.001),LogisticRegression(C=0.01),\n",
    "                                 LogisticRegression(C=0.1),LogisticRegression(C=1),LogisticRegression(C=10),LogisticRegression(C=100),\n",
    "                                 LogisticRegression(C=1000)]}\n",
    "\n",
    "Stacking_grid =StackingClassifier(estimators=estimators,)\n",
    "Stacking_grid_search = GridSearchCV(Stacking_grid, param_grid=param_grid, cv=cvx ,scoring='accuracy',n_jobs=10)\n",
    "Stacking_grid_search.fit(pca_X_train, train_y)\n",
    "Stacking_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a1961530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre_y = cross_val_predict(Stacking_grid_search.best_estimator_, pca_X_train,train_y, cv=cvx)\n",
    "train_res1=get_measures_gridloo(train_y,train_pre_y)\n",
    "#     train_res1['PCA']=n_components\n",
    "\n",
    "test_pre_y = Stacking_grid_search.predict(pca_X_test)\n",
    "test_res1=get_measures_gridloo(test_y,test_pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dfa32e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TP  TN  FP  FN     Sen     Spe     Acc     PPV     NPV     MCC       AUC\n",
      "0  64  50   7   5  0.9275  0.8772  0.9048  0.9014  0.9091  0.8076  0.902365\n",
      "   TP  TN  FP  FN  Sen  Spe  Acc  PPV  NPV  MCC  AUC\n",
      "0  30  24   0   0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "print(train_res1)\n",
    "print(test_res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3da32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shpae:\n",
      "(126, 2281)\n",
      "range(10, 115, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.86090576\n",
      " 0.86090576 0.86090576 0.86090576 0.86090576        nan        nan\n",
      " 0.5               nan 0.5        0.86739927 0.86739927 0.86869797\n",
      " 0.86869797 0.86869797        nan        nan 0.5               nan\n",
      " 0.5        0.89200799 0.89200799 0.89052614 0.89070929 0.89200799\n",
      "        nan        nan 0.84132534        nan 0.83623876 0.89397269\n",
      " 0.89397269 0.89137529 0.89904262 0.89666167        nan        nan\n",
      " 0.94381452        nan 0.94390609 0.91668332 0.91668332 0.92036297\n",
      " 0.92425907 0.92166167        nan        nan 0.92162837        nan\n",
      " 0.92175325 0.91657509 0.91657509 0.92036297 0.91646687 0.91646687\n",
      "        nan        nan 0.91141359        nan 0.91646687 0.90881618\n",
      " 0.90881618 0.90881618 0.91776557 0.91646687        nan        nan\n",
      " 0.90881618        nan 0.91646687 0.90881618 0.90881618 0.90881618\n",
      " 0.91906427 0.91646687        nan        nan 0.90881618        nan\n",
      " 0.91646687 0.90881618 0.90881618 0.90881618 0.91906427 0.91646687]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.88622211\n",
      " 0.88622211 0.88622211 0.88622211 0.88622211        nan        nan\n",
      " 0.5               nan 0.5        0.90047453 0.90047453 0.90177323\n",
      " 0.90177323 0.90177323        nan        nan 0.5               nan\n",
      " 0.5        0.94380619 0.94380619 0.9487679  0.94499667 0.94629537\n",
      "        nan        nan 0.84132534        nan 0.83623876 0.96022311\n",
      " 0.96022311 0.96003996 0.96262071 0.96003996        nan        nan\n",
      " 0.96909757        nan 0.96769064 0.96898934 0.96898934 0.96630037\n",
      " 0.96630037 0.96500167        nan        nan 0.97405927        nan\n",
      " 0.96628372 0.96898934 0.96898934 0.97017982 0.96889777 0.96630037\n",
      "        nan        nan 0.96756577        nan 0.96500167 0.96128871\n",
      " 0.96128871 0.96238761 0.96889777 0.96500167        nan        nan\n",
      " 0.96628372        nan 0.96500167 0.96238761 0.96238761 0.96238761\n",
      " 0.96889777 0.96500167        nan        nan 0.96240426        nan\n",
      " 0.96500167 0.96238761 0.96238761 0.96110556 0.96889777 0.96500167]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.88243423\n",
      " 0.88243423 0.88245088 0.88245088 0.88245088        nan        nan\n",
      " 0.5               nan 0.5        0.90047453 0.90047453 0.90307193\n",
      " 0.90307193 0.90307193        nan        nan 0.5               nan\n",
      " 0.5        0.96173826 0.96173826 0.95937396 0.96054779 0.95935731\n",
      "        nan        nan 0.84132534        nan 0.83623876 0.98056943\n",
      " 0.98056943 0.97800533 0.98056943 0.97800533        nan        nan\n",
      " 0.97535798        nan 0.97665668 0.98058608 0.98058608 0.97545788\n",
      " 0.97675658 0.97802198        nan        nan 0.96113886        nan\n",
      " 0.97802198 0.97156177 0.97156177 0.96899767 0.97286047 0.97803863\n",
      "        nan        nan 0.95860806        nan 0.97673993 0.97027972\n",
      " 0.97027972 0.96380286 0.97286047 0.97803863        nan        nan\n",
      " 0.95730936        nan 0.97673993 0.97157842 0.97157842 0.95471195\n",
      " 0.97286047 0.97803863        nan        nan 0.97159507        nan\n",
      " 0.97803863 0.97027972 0.97027972 0.96120546 0.97286047 0.97803863]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.88239261\n",
      " 0.88239261 0.88109391 0.88109391 0.88109391        nan        nan\n",
      " 0.5               nan 0.5        0.90417083 0.90417083 0.90797536\n",
      " 0.90927406 0.90797536        nan        nan 0.5               nan\n",
      " 0.5        0.95911588 0.95911588 0.95790876 0.95911588 0.95781718\n",
      "        nan        nan 0.84132534        nan 0.83623876 0.98966034\n",
      " 0.98966034 0.98711289 0.98581419 0.98711289        nan        nan\n",
      " 0.97026307        nan 0.96896437 0.98836164 0.98836164 0.98321678\n",
      " 0.98451548 0.98451548        nan        nan 0.96248751        nan\n",
      " 0.98191808 0.98056943 0.98056943 0.98058608 0.98321678 0.98321678\n",
      "        nan        nan 0.95474525        nan 0.98321678 0.97667333\n",
      " 0.97667333 0.97017982 0.98321678 0.98321678        nan        nan\n",
      " 0.94821845        nan 0.98321678 0.97276057 0.97276057 0.96758242\n",
      " 0.98321678 0.98321678        nan        nan 0.945671          nan\n",
      " 0.98321678 0.97146187 0.97146187 0.95597736 0.98321678 0.98321678]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.88755411\n",
      " 0.88755411 0.88636364 0.88636364 0.88636364        nan        nan\n",
      " 0.5               nan 0.5        0.91312021 0.91312021 0.91443556\n",
      " 0.91573427 0.91443556        nan        nan 0.5               nan\n",
      " 0.5        0.96418581 0.96418581 0.96557609 0.96299534 0.96429404\n",
      "        nan        nan 0.84132534        nan 0.83623876 0.98966034\n",
      " 0.98966034 0.98581419 0.98837829 0.98581419        nan        nan\n",
      " 0.96896437        nan 0.97026307 0.98316683 0.98316683 0.98190143\n",
      " 0.98190143 0.98190143        nan        nan 0.95987346        nan\n",
      " 0.97282717 0.97535798 0.97535798 0.96759907 0.98060273 0.98190143\n",
      "        nan        nan 0.93651349        nan 0.98190143 0.96626707\n",
      " 0.96626707 0.95980686 0.98060273 0.98190143        nan        nan\n",
      " 0.94295704        nan 0.98190143 0.96366966 0.96366966 0.951998\n",
      " 0.98060273 0.98190143        nan        nan 0.91510157        nan\n",
      " 0.98190143 0.96107226 0.96107226 0.9494339  0.98060273 0.98190143]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.88625541\n",
      " 0.88625541 0.88495671 0.88495671 0.88495671        nan        nan\n",
      " 0.5               nan 0.5        0.91571762 0.91571762 0.91573427\n",
      " 0.91573427 0.91573427        nan        nan 0.5               nan\n",
      " 0.5        0.96407759 0.96407759 0.96429404 0.96418581 0.96297869\n",
      "        nan        nan 0.84132534        nan 0.83623876 0.98839494\n",
      " 0.98839494 0.98454878 0.98841159 0.98325008        nan        nan\n",
      " 0.96899767        nan 0.97154512 0.98191808 0.98191808 0.97803863\n",
      " 0.98193473 0.98323343        nan        nan 0.9508658         nan\n",
      " 0.97932068 0.96891442 0.96891442 0.97152847 0.97803863 0.98063603\n",
      "        nan        nan 0.93529804        nan 0.98193473 0.96371961\n",
      " 0.96371961 0.95985681 0.97673993 0.98063603        nan        nan\n",
      " 0.93787879        nan 0.98063603 0.96112221 0.96112221 0.95337995\n",
      " 0.97673993 0.98063603        nan        nan 0.94177489        nan\n",
      " 0.98063603 0.96242091 0.95855811 0.94562105 0.97673993 0.98063603]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################random seed loop#################\n",
    "train_aucs=[]\n",
    "test_aucs=[]\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "loopn=10\n",
    "scoring='roc_auc'\n",
    "np.random.seed(10)\n",
    "random_states=np.random.choice(range(101), loopn, replace=False)\n",
    "\n",
    "#print(random_states)\n",
    "for i in range(loopn):\n",
    "    train_X,test_X, train_y, test_y = train_test_split(train,\n",
    "                                                   target,\n",
    "                                                   test_size = 0.3,\n",
    "                                                   stratify=target,\n",
    "                                                   random_state=random_states[i]\n",
    "                                                   )\n",
    "    print(\"train_x.shpae:\")\n",
    "    print(train_X.shape)\n",
    "\n",
    "    standardScaler = StandardScaler()\n",
    "    standardScaler.fit(train_X)\n",
    "    X_standard = standardScaler.transform(train_X)\n",
    "    X_standard_test = standardScaler.transform(test_X)\n",
    "    #calculate max n_components\n",
    "    estimator = PCA(n_components=0.99,random_state=42)\n",
    "    pca_X_train = estimator.fit_transform(X_standard)\n",
    "\n",
    "    n_components=range(10,min(pca_X_train.shape),10)\n",
    "    print(n_components)\n",
    "    #n_components=[0.99,0.95,0.90,0.85]\n",
    "    best_pca_train_aucs=[]\n",
    "    best_pca_test_aucs=[]\n",
    "    \n",
    "    best_pca_train_scores=[]\n",
    "    best_pca_test_scores=[]\n",
    "    for j in n_components:\n",
    "        estimator = PCA(n_components=j,random_state=42)\n",
    "        pca_X_train = estimator.fit_transform(X_standard)\n",
    "        pca_X_test = estimator.transform(X_standard_test)\n",
    "        cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) \n",
    "    \n",
    "        #scoring = {'AUC': 'roc_auc'}\n",
    "        cost = [-5, -3, -1, 1, 3, 5, 7, 9, 11, 13, 15]\n",
    "        gam = [3, 1, -1, -3, -5, -7, -9, -11, -13, -15]\n",
    "        parameters =[{'kernel': ['rbf'], 'C': [2**x for x in cost],'gamma':[2**x for x in gam]}]\n",
    "\n",
    "        svc_grid_search=GridSearchCV(estimator=SVC(random_state=42),\n",
    "                                     param_grid=parameters,cv=cvx,scoring=scoring)\n",
    "        svc_grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "        param_grid = {'penalty':['l1', 'l2'],\n",
    "                      \"C\":[0.00001,0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                      \"solver\":[\"newton-cg\", \"lbfgs\",\"liblinear\",\"sag\",\"saga\"]\n",
    "    #               \"algorithm\":['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "                    }\n",
    "        LR_grid = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        LR_grid_search = GridSearchCV(LR_grid, param_grid=param_grid, cv=cvx ,scoring=scoring,n_jobs=10)\n",
    "        LR_grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "        estimators = [\n",
    "            ('lr', LR_grid_search.best_estimator_),\n",
    "            ('svc', svc_grid_search.best_estimator_),\n",
    "        ]\n",
    "        clf = StackingClassifier(estimators=estimators, \n",
    "                                 final_estimator=LinearSVC(C=5, random_state=42),n_jobs=10)\n",
    "        clf.fit(pca_X_train, train_y)\n",
    "\n",
    "        estimators = [\n",
    "            ('lr', LR_grid_search.best_estimator_),\n",
    "            ('svc', svc_grid_search.best_estimator_),\n",
    "        ]\n",
    "\n",
    "        param_grid = {'final_estimator':[LogisticRegression(C=0.00001),LogisticRegression(C=0.0001),\n",
    "                                         LogisticRegression(C=0.001),LogisticRegression(C=0.01),\n",
    "                                         LogisticRegression(C=0.1),LogisticRegression(C=1),\n",
    "                                         LogisticRegression(C=10),LogisticRegression(C=100),\n",
    "                                         LogisticRegression(C=1000)]}\n",
    "\n",
    "        Stacking_grid =StackingClassifier(estimators=estimators,)\n",
    "        Stacking_grid_search = GridSearchCV(Stacking_grid, param_grid=param_grid, cv=cvx,\n",
    "                                            scoring=scoring,n_jobs=10)\n",
    "        Stacking_grid_search.fit(pca_X_train, train_y)\n",
    "        Stacking_grid_search.best_estimator_\n",
    "\n",
    "        train_pre_y = cross_val_predict(Stacking_grid_search.best_estimator_, pca_X_train,train_y, cv=cvx)\n",
    "        train_res1=get_measures_gridloo(train_y,train_pre_y)\n",
    "\n",
    "        test_pre_y = Stacking_grid_search.predict(pca_X_test)\n",
    "        test_res1=get_measures_gridloo(test_y,test_pre_y)\n",
    "        \n",
    "        best_pca_train_aucs.append(train_res1.loc[:,\"AUC\"])\n",
    "        best_pca_test_aucs.append(test_res1.loc[:,\"AUC\"])\n",
    "        \n",
    "        best_pca_train_scores.append(train_res1)\n",
    "        best_pca_test_scores.append(test_res1)\n",
    "    \n",
    "    train_aucs.append(np.max(best_pca_train_aucs))\n",
    "    test_aucs.append(best_pca_test_aucs[np.argmax(best_pca_train_aucs)].item())\n",
    "    \n",
    "    train_scores.append(best_pca_train_scores[np.argmax(best_pca_train_aucs)])\n",
    "    test_scores.append(best_pca_test_scores[np.argmax(best_pca_train_aucs)])\n",
    "    \n",
    "    print(\"n_components:\")\n",
    "    print(n_components[np.argmax(best_pca_train_aucs)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89e38686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "         TP         TN        FP   FN    Sen       Spe       Acc       PPV  \\\n",
      "count   3.0   3.000000  3.000000  3.0  3.000  3.000000  3.000000  3.000000   \n",
      "mean   67.0  53.333333  3.666667  2.0  0.971  0.935667  0.955033  0.948433   \n",
      "std     0.0   1.527525  1.527525  0.0  0.000  0.026786  0.012166  0.020612   \n",
      "min    67.0  52.000000  2.000000  2.0  0.971  0.912300  0.944400  0.930600   \n",
      "25%    67.0  52.500000  3.000000  2.0  0.971  0.921050  0.948400  0.937150   \n",
      "50%    67.0  53.000000  4.000000  2.0  0.971  0.929800  0.952400  0.943700   \n",
      "75%    67.0  54.000000  4.500000  2.0  0.971  0.947350  0.960350  0.957350   \n",
      "max    67.0  55.000000  5.000000  2.0  0.971  0.964900  0.968300  0.971000   \n",
      "\n",
      "            NPV       MCC       AUC  \n",
      "count  3.000000  3.000000  3.000000  \n",
      "mean   0.963833  0.909467  0.953344  \n",
      "std    0.000971  0.024200  0.013399  \n",
      "min    0.963000  0.888400  0.941648  \n",
      "25%    0.963300  0.896250  0.946034  \n",
      "50%    0.963600  0.904100  0.950420  \n",
      "75%    0.964250  0.920000  0.959191  \n",
      "max    0.964900  0.935900  0.967963  \n",
      "test\n",
      "              TP         TN        FP        FN       Sen       Spe      Acc  \\\n",
      "count   3.000000   3.000000  3.000000  3.000000  3.000000  3.000000  3.00000   \n",
      "mean   27.666667  22.333333  1.666667  2.333333  0.922233  0.930567  0.92590   \n",
      "std     1.154701   0.577350  0.577350  1.154701  0.038509  0.024018  0.01850   \n",
      "min    27.000000  22.000000  1.000000  1.000000  0.900000  0.916700  0.90740   \n",
      "25%    27.000000  22.000000  1.500000  2.000000  0.900000  0.916700  0.91665   \n",
      "50%    27.000000  22.000000  2.000000  3.000000  0.900000  0.916700  0.92590   \n",
      "75%    28.000000  22.500000  2.000000  3.000000  0.933350  0.937500  0.93515   \n",
      "max    29.000000  23.000000  2.000000  3.000000  0.966700  0.958300  0.94440   \n",
      "\n",
      "            PPV       NPV       MCC       AUC  \n",
      "count  3.000000  3.000000  3.000000  3.000000  \n",
      "mean   0.943600  0.907033  0.851700  0.926389  \n",
      "std    0.018067  0.042901  0.036987  0.016839  \n",
      "min    0.931000  0.880000  0.813800  0.908333  \n",
      "25%    0.933250  0.882300  0.833700  0.918750  \n",
      "50%    0.935500  0.884600  0.853600  0.929167  \n",
      "75%    0.949900  0.920550  0.870650  0.935417  \n",
      "max    0.964300  0.956500  0.887700  0.941667  \n"
     ]
    }
   ],
   "source": [
    "# print('mean')\n",
    "# print(np.mean(train_scores))\n",
    "# print(np.mean(test_scores))\n",
    "# print(\"std\")\n",
    "# print(np.std(train_scores))\n",
    "# print(np.std(test_scores))\n",
    "# pd.DataFrame(tmp_train)\n",
    "# print(tmp_train)\n",
    "# print(test_scores)\n",
    "# print(train_scores)\n",
    "# print(test_scores)\n",
    "print(\"train\")\n",
    "print(pd.concat(train_scores).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f575b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")\n",
    "print(pd.concat(test_scores).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac12068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# s_train=sorted(train_scores)\n",
    "# s_test=sorted(test_scores)\n",
    "\n",
    "scores=list(range(len(train_scores)))\n",
    "for i in range(len(train_scores)):\n",
    "    scores[i]=[train_scores[i],test_scores[i]]\n",
    "\n",
    "scores.sort(key=lambda x:x[0])\n",
    "\n",
    "s_train=list(range(len(train_scores)))\n",
    "s_test=list(range(len(train_scores)))\n",
    "for i in range(len(train_scores)):\n",
    "    s_train[i]=scores[i][0]\n",
    "    s_test[i]=scores[i][1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(s_train)\n",
    "plt.plot(s_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "1d361042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.909643605870021, 0.9210526315789473],\n",
       " [0.909643605870021, 0.868421052631579],\n",
       " [0.9144654088050315, 0.8802440884820747],\n",
       " [0.9318658280922432, 0.9591914569031272],\n",
       " [0.9381551362683438, 0.9183829138062548],\n",
       " [0.9429769392033542, 0.8802440884820747],\n",
       " [0.9461215932914046, 0.8947368421052632],\n",
       " [0.9492662473794549, 0.9065598779557589],\n",
       " [0.9492662473794549, 0.8421052631578947],\n",
       " [0.9763102725366877, 0.9183829138062548]]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "944e7062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shpae:\n",
      "(89, 38585)\n",
      "range(10, 85, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.99047619\n",
      " 0.99047619 0.99047619 0.99047619 0.99047619        nan        nan\n",
      " 0.5               nan 0.5        0.99047619 0.99047619 0.9702381\n",
      " 0.9702381  0.9702381         nan        nan 0.89047619        nan\n",
      " 0.89047619 0.99047619 0.99047619 0.95238095 0.95238095 0.95238095\n",
      "        nan        nan 0.9797619         nan 0.9797619  0.99047619\n",
      " 0.99047619 0.94880952 0.94880952 0.94880952        nan        nan\n",
      " 0.95833333        nan 0.95833333 0.99047619 0.99047619 0.9452381\n",
      " 0.93722222 0.9452381         nan        nan 0.93809524        nan\n",
      " 0.9452381  0.98690476 0.98690476 0.93007937 0.93722222 0.94079365\n",
      "        nan        nan 0.93809524        nan 0.94079365 0.98690476\n",
      " 0.98690476 0.92531746 0.93722222 0.94079365        nan        nan\n",
      " 0.93809524        nan 0.94079365 0.98333333 0.98333333 0.92888889\n",
      " 0.93722222 0.94079365        nan        nan 0.94761905        nan\n",
      " 0.94079365 0.9797619  0.98333333 0.92888889 0.93722222 0.94079365]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.99047619\n",
      " 0.99047619 0.99047619 0.99047619 0.99047619        nan        nan\n",
      " 0.5               nan 0.5        0.9952381  0.9952381  0.96904762\n",
      " 0.97261905 0.97261905        nan        nan 0.89047619        nan\n",
      " 0.89047619 1.         1.         0.95238095 0.95238095 0.95238095\n",
      "        nan        nan 0.9797619         nan 0.9797619  1.\n",
      " 1.         0.94857143 0.945      0.945             nan        nan\n",
      " 0.93547619        nan 0.93190476 1.         1.         0.945\n",
      " 0.945      0.94142857        nan        nan 0.945             nan\n",
      " 0.94142857 1.         1.         0.95214286 0.945      0.94142857\n",
      "        nan        nan 0.94142857        nan 0.94142857 1.\n",
      " 1.         0.95214286 0.945      0.94142857        nan        nan\n",
      " 0.94142857        nan 0.94142857 1.         1.         0.95214286\n",
      " 0.945      0.94142857        nan        nan 0.93785714        nan\n",
      " 0.94142857 1.         1.         0.94857143 0.945      0.94142857]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.9952381\n",
      " 0.9952381  0.99047619 0.99047619 0.99047619        nan        nan\n",
      " 0.5               nan 0.5        1.         1.         0.97380952\n",
      " 0.97380952 0.97380952        nan        nan 0.89047619        nan\n",
      " 0.89047619 1.         1.         0.95357143 0.95357143 0.95357143\n",
      "        nan        nan 0.9797619         nan 0.9797619  1.\n",
      " 1.         0.89626984 0.89150794 0.89269841        nan        nan\n",
      " 0.93634921        nan 0.94349206 1.         1.         0.87007937\n",
      " 0.87484127 0.87484127        nan        nan 0.90753968        nan\n",
      " 0.90515873 1.         1.         0.87809524 0.87007937 0.87928571\n",
      "        nan        nan 0.91738095        nan 0.88404762 1.\n",
      " 1.         0.86531746 0.87007937 0.87928571        nan        nan\n",
      " 0.88047619        nan 0.87928571 1.         1.         0.82753968\n",
      " 0.87007937 0.87928571        nan        nan 0.90690476        nan\n",
      " 0.87928571 1.         1.         0.81952381 0.87007937 0.87928571]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.9952381\n",
      " 0.9952381  0.99047619 0.99047619 0.99047619        nan        nan\n",
      " 0.5               nan 0.5        1.         1.         0.97380952\n",
      " 0.97738095 0.97738095        nan        nan 0.89047619        nan\n",
      " 0.89047619 1.         1.         0.96547619 0.96547619 0.96547619\n",
      "        nan        nan 0.9797619         nan 0.9797619  1.\n",
      " 1.         0.9334127  0.94230159 0.94230159        nan        nan\n",
      " 0.94079365        nan 0.95714286 1.         1.         0.91738095\n",
      " 0.92539683 0.93785714        nan        nan 0.93785714        nan\n",
      " 0.94944444 1.         1.         0.90071429 0.92539683 0.93428571\n",
      "        nan        nan 0.92539683        nan 0.93785714 1.\n",
      " 1.         0.89777778 0.92984127 0.93428571        nan        nan\n",
      " 0.90785714        nan 0.93785714 1.         1.         0.83238095\n",
      " 0.92984127 0.93428571        nan        nan 0.90333333        nan\n",
      " 0.93428571 1.         0.94761905 0.82436508 0.92984127 0.93428571]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.99047619\n",
      " 0.99047619 0.99047619 0.99047619 0.99047619        nan        nan\n",
      " 0.5               nan 0.5        1.         1.         0.96428571\n",
      " 0.96428571 0.96428571        nan        nan 0.89047619        nan\n",
      " 0.89047619 1.         1.         0.95714286 0.95714286 0.95714286\n",
      "        nan        nan 0.9797619         nan 0.9797619  1.\n",
      " 1.         0.94047619 0.95       0.95              nan        nan\n",
      " 0.96309524        nan 0.95714286 1.         1.         0.90126984\n",
      " 0.93333333 0.93690476        nan        nan 0.93277778        nan\n",
      " 0.95357143 1.         1.         0.8968254  0.93333333 0.93690476\n",
      "        nan        nan 0.92920635        nan 0.94642857 1.\n",
      " 1.         0.88436508 0.93333333 0.93690476        nan        nan\n",
      " 0.9152381         nan 0.93690476 1.         1.         0.8418254\n",
      " 0.93333333 0.93690476        nan        nan 0.89349206        nan\n",
      " 0.93690476 1.         0.87722222 0.83468254 0.93333333 0.93690476]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.99047619\n",
      " 0.99047619 0.99047619 0.99047619 0.99047619        nan        nan\n",
      " 0.5               nan 0.5        1.         1.         0.96785714\n",
      " 0.96785714 0.96785714        nan        nan 0.89047619        nan\n",
      " 0.89047619 1.         1.         0.95714286 0.95714286 0.95714286\n",
      "        nan        nan 0.9797619         nan 0.9797619  1.\n",
      " 1.         0.94642857 0.95357143 0.95357143        nan        nan\n",
      " 0.96309524        nan 0.96666667 1.         1.         0.93396825\n",
      " 0.94642857 0.94642857        nan        nan 0.9452381         nan\n",
      " 0.95357143 1.         1.         0.92119048 0.94642857 0.94642857\n",
      "        nan        nan 0.92563492        nan 0.94642857 1.\n",
      " 1.         0.91285714 0.94642857 0.94642857        nan        nan\n",
      " 0.91849206        nan 0.94642857 1.         1.         0.76666667\n",
      " 0.94642857 0.94642857        nan        nan 0.86468254        nan\n",
      " 0.94642857 1.         0.90904762 0.75507937 0.94642857 0.94642857]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.9952381\n",
      " 0.9952381  0.99047619 0.99047619 0.99047619        nan        nan\n",
      " 0.5               nan 0.5        1.         1.         0.96785714\n",
      " 0.96785714 0.96785714        nan        nan 0.89047619        nan\n",
      " 0.89047619 1.         1.         0.95714286 0.95714286 0.95714286\n",
      "        nan        nan 0.9797619         nan 0.9797619  1.\n",
      " 1.         0.95714286 0.95714286 0.95714286        nan        nan\n",
      " 0.96309524        nan 0.96666667 1.         1.         0.95357143\n",
      " 0.95714286 0.95714286        nan        nan 0.94880952        nan\n",
      " 0.95714286 1.         1.         0.94880952 0.95714286 0.95714286\n",
      "        nan        nan 0.9452381         nan 0.95714286 1.\n",
      " 1.         0.93333333 0.95714286 0.95714286        nan        nan\n",
      " 0.93095238        nan 0.95714286 1.         1.         0.78738095\n",
      " 0.95714286 0.95714286        nan        nan 0.76063492        nan\n",
      " 0.95714286 1.         0.95       0.76690476 0.95714286 0.95714286]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.9952381\n",
      " 0.9952381  0.99047619 0.99047619 0.99047619        nan        nan\n",
      " 0.5               nan 0.5        1.         1.         0.97261905\n",
      " 0.97261905 0.97261905        nan        nan 0.89047619        nan\n",
      " 0.89047619 1.         1.         0.96071429 0.96071429 0.96071429\n",
      "        nan        nan 0.9797619         nan 0.9797619  1.\n",
      " 1.         0.96071429 0.96071429 0.96071429        nan        nan\n",
      " 0.96666667        nan 0.96666667 1.         1.         0.95714286\n",
      " 0.96071429 0.96071429        nan        nan 0.96666667        nan\n",
      " 0.96071429 1.         1.         0.95714286 0.96071429 0.96071429\n",
      "        nan        nan 0.96666667        nan 0.96071429 1.\n",
      " 1.         0.95357143 0.96071429 0.96071429        nan        nan\n",
      " 0.93571429        nan 0.96071429 1.         1.         0.82420635\n",
      " 0.96071429 0.96071429        nan        nan 0.89865079        nan\n",
      " 0.96071429 1.         0.95714286 0.79333333 0.96071429 0.96071429]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "random_stat=10\n",
    "train_X,test_X, train_y, test_y = train_test_split(train,\n",
    "                                                   target,\n",
    "                                                   test_size = 0.3,\n",
    "                                                   stratify=target,\n",
    "                                                   random_state=random_stat\n",
    "                                                   )\n",
    "print(\"train_x.shpae:\")\n",
    "print(train_X.shape)\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "standardScaler.fit(train_X)\n",
    "X_standard = standardScaler.transform(train_X)\n",
    "X_standard_test = standardScaler.transform(test_X)\n",
    "    #calculate max n_components\n",
    "estimator = PCA(n_components=0.99,random_state=42)\n",
    "pca_X_train = estimator.fit_transform(X_standard)\n",
    "\n",
    "n_components=range(10,min(pca_X_train.shape),10)\n",
    "print(n_components)\n",
    "    #n_components=[0.99,0.95,0.90,0.85]\n",
    "best_pca_train_scores=[]\n",
    "best_pca_test_scores=[]\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "for j in n_components:\n",
    "    estimator = PCA(n_components=j,random_state=42)\n",
    "    pca_X_train = estimator.fit_transform(X_standard)\n",
    "    pca_X_test = estimator.transform(X_standard_test)\n",
    "    cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) \n",
    "    \n",
    "        #scoring = {'AUC': 'roc_auc'}\n",
    "    cost = [-5, -3, -1, 1, 3, 5, 7, 9, 11, 13, 15]\n",
    "    gam = [3, 1, -1, -3, -5, -7, -9, -11, -13, -15]\n",
    "    parameters =[{'kernel': ['rbf'], 'C': [2**x for x in cost],'gamma':[2**x for x in gam]}]\n",
    "\n",
    "    svc_grid_search=GridSearchCV(estimator=SVC(random_state=42),\n",
    "                                 param_grid=parameters,cv=cvx,scoring=scoring)\n",
    "    svc_grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "    param_grid = {'penalty':['l1', 'l2'],\n",
    "                  \"C\":[0.00001,0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                  \"solver\":[\"newton-cg\", \"lbfgs\",\"liblinear\",\"sag\",\"saga\"]\n",
    "    #               \"algorithm\":['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "                  }\n",
    "    LR_grid = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    LR_grid_search = GridSearchCV(LR_grid, param_grid=param_grid, cv=cvx ,scoring=scoring,n_jobs=10)\n",
    "    LR_grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "    estimators = [\n",
    "        ('lr', LR_grid_search.best_estimator_),\n",
    "        ('svc', svc_grid_search.best_estimator_),\n",
    "    ]\n",
    "    clf = StackingClassifier(estimators=estimators, \n",
    "                             final_estimator=LinearSVC(C=5, random_state=42),n_jobs=10)\n",
    "    clf.fit(pca_X_train, train_y)\n",
    "\n",
    "    estimators = [\n",
    "        ('lr', LR_grid_search.best_estimator_),\n",
    "        ('svc', svc_grid_search.best_estimator_),\n",
    "    ]\n",
    "\n",
    "    param_grid = {'final_estimator':[LogisticRegression(C=0.00001),LogisticRegression(C=0.0001),\n",
    "                                    LogisticRegression(C=0.001),LogisticRegression(C=0.01),\n",
    "                                    LogisticRegression(C=0.1),LogisticRegression(C=1),\n",
    "                                    LogisticRegression(C=10),LogisticRegression(C=100),\n",
    "                                    LogisticRegression(C=1000)]}\n",
    "\n",
    "    Stacking_grid =StackingClassifier(estimators=estimators,)\n",
    "    Stacking_grid_search = GridSearchCV(Stacking_grid, param_grid=param_grid, cv=cvx,\n",
    "                                        scoring=scoring,n_jobs=10)\n",
    "    Stacking_grid_search.fit(pca_X_train, train_y)\n",
    "    Stacking_grid_search.best_estimator_\n",
    "\n",
    "    train_pre_y = cross_val_predict(Stacking_grid_search.best_estimator_, pca_X_train,train_y, cv=cvx)\n",
    "    train_res1=get_measures_gridloo(train_y,train_pre_y)\n",
    "\n",
    "    test_pre_y = Stacking_grid_search.predict(pca_X_test)\n",
    "    test_res1=get_measures_gridloo(test_y,test_pre_y)\n",
    "        \n",
    "    best_pca_train_scores.append(train_res1.loc[:,\"AUC\"])\n",
    "    best_pca_test_scores.append(test_res1.loc[:,\"AUC\"])\n",
    "    train_scores.append(train_res1)\n",
    "    test_scores.append(test_res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "8799ba25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_AUC:\n",
      "0.5\n",
      "test_AUC:\n",
      "0    0.5\n",
      "Name: AUC, dtype: float64\n",
      "n_components:\n",
      "10\n",
      "train_scores:\n",
      "   TP  TN  FP  FN  Sen  Spe    Acc  PPV    NPV  MCC  AUC\n",
      "0   0  72   0  17  0.0  1.0  0.809  NaN  0.809  NaN  0.5\n",
      "test_scores:\n",
      "   TP  TN  FP  FN  Sen  Spe     Acc  PPV     NPV  MCC  AUC\n",
      "0   0  31   0   8  0.0  1.0  0.7949  NaN  0.7949  NaN  0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"train_AUC:\")\n",
    "print(np.max(best_pca_train_scores))\n",
    "print(\"test_AUC:\")\n",
    "print(best_pca_test_scores[np.argmax(best_pca_train_scores)])\n",
    "print(\"n_components:\")\n",
    "print(n_components[np.argmax(best_pca_train_scores)])\n",
    "\n",
    "print(\"train_scores:\")\n",
    "print(train_scores[np.argmax(best_pca_train_scores)])\n",
    "print(\"test_scores:\")\n",
    "print(test_scores[np.argmax(best_pca_train_scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62414dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
