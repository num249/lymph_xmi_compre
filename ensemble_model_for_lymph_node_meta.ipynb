{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbef4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, cross_val_predict\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.neighbors import KNeighborsClassifier   \n",
    "from sklearn import svm  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "import datetime\n",
    "import multiprocessing as mp\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "# cores = mp.cpu_count()\n",
    "# pool = mp.Pool(processes=20)\n",
    "\n",
    "# min1=min(list(X_standard.shape))\n",
    "# a1=list(range(10,min1,10))\n",
    "# a1[-1]=min1\n",
    "# pcc_top1=pool.map(find_best_pca_lr,a1)  ##（SVM，LR）\n",
    "# pool.terminate()\n",
    "\n",
    "\n",
    "# a1=pd.DataFrame()\n",
    "# for i1 in pcc_top1:\n",
    "#     a1=pd.concat([a1,i1], axis=0)\n",
    "# a1.columns=['tr_TP', 'tr_TN', 'tr_FP', 'tr_FN', 'tr_Sen', 'tr_Spe', 'tr_Acc', 'tr_PPV', 'tr_NPV', 'tr_MCC', 'tr_AUC','TP',\n",
    "#        'TN', 'FP', 'FN', 'Sen', 'Spe', 'Acc', 'PPV', 'NPV', 'MCC', 'AUC','PCA']   \n",
    "\n",
    "# a1=a1.sort_values(['Acc','tr_Acc','PCA'], ascending=[False,False,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a00ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_csv(input_path,i):\n",
    "\n",
    "    df_chunk=pd.read_csv(input_path,\n",
    "                         chunksize=10000,\n",
    "#                          sep='\\t'\n",
    "                        sep=i\n",
    "                        )\n",
    "    res_chunk=[]\n",
    "    for chunk in df_chunk:\n",
    "        res_chunk.append(chunk)\n",
    "    res_df=pd.concat(res_chunk)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d3ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measures_gridloo(label, score):\n",
    "    label = np.array(label)\n",
    "    score = np.array(score)\n",
    "    \n",
    "    N  = len(label)\n",
    "    TP = sum((label == 1) & (score == 1))\n",
    "    TN = sum((label == 0) & (score == 0))\n",
    "    FP = sum((label == 0) & (score == 1))\n",
    "    FN = sum((label == 1) & (score == 0))\n",
    "\n",
    "    # init all measures to nan\n",
    "    measures = {measure: float(\"nan\") for measure in (\"Sen\", \"Spe\", \"Acc\", \"PPV\", \"NPV\", \"MCC\",\"AUC\")}\n",
    "    \n",
    "    measures[\"TP\"] = TP\n",
    "    measures[\"TN\"] = TN\n",
    "    measures[\"FP\"] = FP\n",
    "    measures[\"FN\"] = FN\n",
    "    \n",
    "    S = (TP + FN) / N\n",
    "    P = (TP + FP) / N\n",
    "\n",
    "    if (TP + FN) > 0:\n",
    "        measures[\"Sen\"] = round(TP/(TP+FN), 4)\n",
    "\n",
    "    if (TN + FP) > 0:\n",
    "        measures[\"Spe\"] = round(TN/(TN+FP), 4)\n",
    "\n",
    "    if (TP + FP + FN + TN) > 0:\n",
    "        measures[\"Acc\"] = round((TP+TN)/(TP+FP+FN+TN), 4)\n",
    "\n",
    "    if (TP + FP) > 0:\n",
    "        measures[\"PPV\"] = round(TP/(TP+FP), 4)\n",
    "\n",
    "    if (TN + FN) > 0:\n",
    "        measures[\"NPV\"] = round(TN/(TN+FN), 4)\n",
    "\n",
    "    if (S*P*(1-S)*(1-P)) > 0:\n",
    "        measures[\"MCC\"] = round((TP/N - S*P)/(math.sqrt(S*P*(1-S)*(1-P))), 4)\n",
    "    \n",
    "    \n",
    "    measures[\"AUC\"]= roc_auc_score(label, score)\n",
    "    return pd.DataFrame([measures],\n",
    "                        columns=[\"TP\", \"TN\", \"FP\",\n",
    "                                 \"FN\", \"Sen\", \"Spe\", \"Acc\", \"PPV\", \"NPV\", \"MCC\",\"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d53b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SVM\n",
    "def find_best_pca(i):\n",
    "    n_components=i\n",
    "    estimator = PCA(n_components=n_components,random_state=10)\n",
    "    pca_X_train = estimator.fit_transform(X_standard)\n",
    "    pca_X_test = estimator.transform(X_standard_test)\n",
    "    \n",
    "    scoring = {'AUC': 'roc_auc'}\n",
    "    cost = [-5, -3, -1, 1, 3, 5, 7, 9, 11, 13, 15]\n",
    "    gam = [3, 1, -1, -3, -5, -7, -9, -11, -13, -15]\n",
    "    parameters =[{'kernel': ['rbf'], 'C': [2**x for x in cost],'gamma':[2**x for x in gam]}]\n",
    "    \n",
    "    cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=10) \n",
    "    grid_search=GridSearchCV(estimator=SVC(random_state=10),param_grid=parameters,cv=cvx,scoring='accuracy')\n",
    "    grid_search.fit(pca_X_train, train_y)\n",
    "    \n",
    "    train_pre_y = cross_val_predict(grid_search.best_estimator_, pca_X_train,train_y, cv=cvx)\n",
    "    train_res1=get_measures_gridloo(train_y,train_pre_y)\n",
    "#     train_res1['PCA']=n_components\n",
    "    test_pre_y = grid_search.predict(pca_X_test)\n",
    "    test_res1=get_measures_gridloo(test_y,test_pre_y)\n",
    "    test_res1['PCA']=n_components\n",
    "    \n",
    "    res=pd.concat([train_res1,test_res1], axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4e5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_pca_lr(i):\n",
    "    n_components=i\n",
    "    estimator = PCA(n_components=n_components,random_state=10)\n",
    "    pca_X_train = estimator.fit_transform(X_standard)\n",
    "    pca_X_test = estimator.transform(X_standard_test)\n",
    "    \n",
    "    cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=10) \n",
    "\n",
    "    param_grid = {'penalty':['l2'],\n",
    "              \"C\":[0.00001,0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000],  \n",
    "              \"solver\":[\"newton-cg\", \"lbfgs\",\"liblinear\"],\n",
    "#             \"C\":list(np.arange(0,0.1,0.005)),   \n",
    "#               \"solver\":[\"newton-cg\", \"lbfgs\",\"liblinear\"]\n",
    "            }\n",
    "    LR_grid = LogisticRegression()\n",
    "    grid_search = GridSearchCV(LR_grid, param_grid=param_grid, cv=cvx ,scoring='accuracy',n_jobs=9)\n",
    "    grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "\n",
    "    train_pre_y = cross_val_predict(grid_search.best_estimator_, pca_X_train,train_y, cv=cvx)\n",
    "    train_res1=get_measures_gridloo(train_y,train_pre_y)\n",
    "#     train_res1['PCA']=n_components\n",
    "   \n",
    "\n",
    "    test_pre_y = grid_search.predict(pca_X_test)\n",
    "    test_res1=get_measures_gridloo(test_y,test_pre_y)\n",
    "    test_res1['PCA']=n_components\n",
    "    \n",
    "    res=pd.concat([train_res1,test_res1], axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5f27eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer='BRCA'\n",
    "path='D:\\\\lymph_meta_xmiseq\\\\tcga_data\\\\'\n",
    "# e1=\"./\"+cancer+\"_delta_pcc_wilcox.csv\"\n",
    "# delta_pcc=read_single_csv(e1,',')\n",
    "# delta_pcc=delta_pcc.drop(['Unnamed: 0','pair'],axis=1)\n",
    "# delta_pcc=delta_pcc.dropna(axis=0,how='any')\n",
    "\n",
    "# result=delta_pcc.T\n",
    "# train=result.iloc[:,0:-1].values.astype('float')\n",
    "# target=result.iloc[:,-1].values.astype('float')\n",
    "\n",
    "train=pd.read_csv(path+cancer+\"\\\\admat_final.csv\").iloc[:,2:].T.values\n",
    "target=pd.read_csv(path+cancer+\"\\\\gctm_label.csv\",index_col=0).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "986f9b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00874834, -0.00736022, -0.01710605, ...,  0.0059139 ,\n",
       "         0.00585509,  0.01760065],\n",
       "       [-0.06756009, -0.03635654, -0.06104614, ...,  0.00184041,\n",
       "         0.01073223, -0.00660949],\n",
       "       [ 0.05017487, -0.03385318, -0.05728117, ..., -0.01067972,\n",
       "         0.01398897,  0.01295414],\n",
       "       ...,\n",
       "       [-0.00012984,  0.00158047,  0.00504004, ...,  0.00526114,\n",
       "        -0.00421565, -0.0168382 ],\n",
       "       [-0.0120549 ,  0.0223974 , -0.02588812, ..., -0.03524445,\n",
       "         0.00812885,  0.01039054],\n",
       "       [ 0.00757404,  0.00548185,  0.02304116, ...,  0.00104388,\n",
       "         0.00138618, -0.0349668 ]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a0046d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4d47e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,test_X, train_y, test_y = train_test_split(train,\n",
    "                                                   target,\n",
    "                                                   test_size = 0.3,\n",
    "                                                   stratify=target,\n",
    "                                                   random_state=10\n",
    "                                                   )\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "standardScaler.fit(train_X)\n",
    "X_standard = standardScaler.transform(train_X)\n",
    "X_standard_test = standardScaler.transform(test_X)\n",
    "\n",
    "n_components=0.95\n",
    "estimator = PCA(n_components=n_components,random_state=10)\n",
    "pca_X_train = estimator.fit_transform(X_standard)\n",
    "pca_X_test = estimator.transform(X_standard_test)\n",
    "cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4d3eaf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 2281)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "385399ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.45230769        nan 0.52369231 0.54769231\n",
      " 0.54769231 0.72153846 0.76123077 0.74523077        nan        nan\n",
      " 0.45230769        nan 0.50030769 0.75292308 0.75292308 0.75292308\n",
      " 0.79230769 0.75292308        nan        nan 0.45230769        nan\n",
      " 0.52369231 0.848      0.848      0.82430769 0.864      0.832\n",
      "        nan        nan 0.68246154        nan 0.75292308 0.888\n",
      " 0.888      0.87230769 0.88       0.88              nan        nan\n",
      " 0.92              nan 0.92       0.88061538 0.88061538 0.85661538\n",
      " 0.86461538 0.86461538        nan        nan 0.944             nan\n",
      " 0.936      0.84061538 0.84061538 0.86461538 0.86461538 0.87261538\n",
      "        nan        nan 0.944             nan 0.87261538 0.84861538\n",
      " 0.84861538 0.86461538 0.86461538 0.87261538        nan        nan\n",
      " 0.90430769        nan 0.87261538 0.85661538 0.85661538 0.84861538\n",
      " 0.86461538 0.87261538        nan        nan 0.80892308        nan\n",
      " 0.87261538 0.84861538 0.84861538 0.84061538 0.86461538 0.87261538]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=10, shuffle=True),\n",
       "             estimator=LogisticRegression(max_iter=1000), n_jobs=10,\n",
       "             param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100,\n",
       "                               1000],\n",
       "                         'penalty': ['l1', 'l2'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#svm\n",
    "scoring = {'AUC': 'roc_auc'}\n",
    "cost = [-5, -3, -1, 1, 3, 5, 7, 9, 11, 13, 15]\n",
    "gam = [3, 1, -1, -3, -5, -7, -9, -11, -13, -15]\n",
    "parameters =[{'kernel': ['rbf'], 'C': [2**x for x in cost],'gamma':[2**x for x in gam]}]\n",
    "cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=10) \n",
    "\n",
    "svc_grid_search=GridSearchCV(estimator=SVC(random_state=10),param_grid=parameters,cv=cvx,scoring='accuracy')\n",
    "svc_grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "#LR\n",
    "param_grid = {'penalty':['l1', 'l2'],\n",
    "              \"C\":[0.00001,0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              \"solver\":[\"newton-cg\", \"lbfgs\",\"liblinear\",\"sag\",\"saga\"]\n",
    "#               \"algorithm\":['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "            }\n",
    "LR_grid = LogisticRegression(max_iter=1000)\n",
    "LR_grid_search = GridSearchCV(LR_grid, param_grid=param_grid, cv=cvx ,scoring='accuracy',n_jobs=10)\n",
    "LR_grid_search.fit(pca_X_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c0fc0d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('lr',\n",
       "                                LogisticRegression(C=1, max_iter=1000,\n",
       "                                                   penalty='l1',\n",
       "                                                   solver='liblinear')),\n",
       "                               ('svc',\n",
       "                                SVC(C=32, gamma=3.0517578125e-05,\n",
       "                                    random_state=10))],\n",
       "                   final_estimator=LinearSVC(C=5), n_jobs=10)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('lr', LR_grid_search.best_estimator_),\n",
    "    ('svc', svc_grid_search.best_estimator_),\n",
    "]\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LinearSVC(C=5),n_jobs=10)\n",
    "clf.fit(pca_X_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "aa318404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('lr',\n",
       "                                LogisticRegression(C=1, max_iter=1000,\n",
       "                                                   penalty='l1',\n",
       "                                                   solver='liblinear')),\n",
       "                               ('svc',\n",
       "                                SVC(C=32, gamma=3.0517578125e-05,\n",
       "                                    random_state=10))],\n",
       "                   final_estimator=LogisticRegression(C=100))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('lr', LR_grid_search.best_estimator_),\n",
    "    ('svc', svc_grid_search.best_estimator_),\n",
    "]\n",
    "\n",
    "param_grid = {'final_estimator':[LogisticRegression(C=0.00001),LogisticRegression(C=0.0001),LogisticRegression(C=0.001),LogisticRegression(C=0.01),\n",
    "                                 LogisticRegression(C=0.1),LogisticRegression(C=1),LogisticRegression(C=10),LogisticRegression(C=100),\n",
    "                                 LogisticRegression(C=1000)]}\n",
    "\n",
    "Stacking_grid =StackingClassifier(estimators=estimators,)\n",
    "Stacking_grid_search = GridSearchCV(Stacking_grid, param_grid=param_grid, cv=cvx ,scoring='accuracy',n_jobs=10)\n",
    "Stacking_grid_search.fit(pca_X_train, train_y)\n",
    "Stacking_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a1961530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre_y = cross_val_predict(Stacking_grid_search.best_estimator_, pca_X_train,train_y, cv=cvx)\n",
    "train_res1=get_measures_gridloo(train_y,train_pre_y)\n",
    "#     train_res1['PCA']=n_components\n",
    "\n",
    "test_pre_y = Stacking_grid_search.predict(pca_X_test)\n",
    "test_res1=get_measures_gridloo(test_y,test_pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dfa32e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TP  TN  FP  FN     Sen     Spe     Acc     PPV     NPV     MCC       AUC\n",
      "0  64  50   7   5  0.9275  0.8772  0.9048  0.9014  0.9091  0.8076  0.902365\n",
      "   TP  TN  FP  FN  Sen  Spe  Acc  PPV  NPV  MCC  AUC\n",
      "0  30  24   0   0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "print(train_res1)\n",
    "print(test_res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ac3da32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shpae:\n",
      "(195, 13022)\n",
      "range(10, 132, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.79777778\n",
      " 0.79777778 0.79778056 0.79778056 0.79778056        nan        nan\n",
      " 0.5               nan 0.5        0.80732387 0.80732387 0.80944027\n",
      " 0.80944027 0.80944027        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.80894737 0.80894737 0.81001392 0.80948482 0.80948482\n",
      "        nan        nan 0.80202172        nan 0.80202172 0.81582568\n",
      " 0.81582568 0.81529936 0.81529936 0.81477304        nan        nan\n",
      " 0.81265664        nan 0.81370927 0.8121359  0.8121359  0.81319131\n",
      " 0.8121359  0.812665          nan        nan 0.81477304        nan\n",
      " 0.81318853 0.8131941  0.8131941  0.8131941  0.812665   0.812665\n",
      "        nan        nan 0.8131941         nan 0.812665   0.8137232\n",
      " 0.8137232  0.8137232  0.8131941  0.812665          nan        nan\n",
      " 0.8131941         nan 0.812665   0.8137232  0.8137232  0.8137232\n",
      " 0.8131941  0.812665          nan        nan 0.8131941         nan\n",
      " 0.812665   0.8137232  0.8137232  0.8137232  0.8131941  0.812665  ]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.80939293\n",
      " 0.80939293 0.80939293 0.80939293 0.80939293        nan        nan\n",
      " 0.5               nan 0.5        0.832665   0.832665   0.83161793\n",
      " 0.83161793 0.83161793        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.83691172 0.83691172 0.83637984 0.83637984 0.83637984\n",
      "        nan        nan 0.81314119        nan 0.81314119 0.85644946\n",
      " 0.85644946 0.85644946 0.85750487 0.85644946        nan        nan\n",
      " 0.84642439        nan 0.84695071 0.87281259 0.87281259 0.86912838\n",
      " 0.86595934 0.86226399        nan        nan 0.87335004        nan\n",
      " 0.86120579 0.87756057 0.87756057 0.8802005  0.86965748 0.86331941\n",
      "        nan        nan 0.87755778        nan 0.86331941 0.87755778\n",
      " 0.87755778 0.8780841  0.87018658 0.86331941        nan        nan\n",
      " 0.87755778        nan 0.86331941 0.87755778 0.87755778 0.87755778\n",
      " 0.87018658 0.86331941        nan        nan 0.87755778        nan\n",
      " 0.86331941 0.87755778 0.87755778 0.87755778 0.87018658 0.86331941]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.81468672\n",
      " 0.81468672 0.8141604  0.8141604  0.8141604         nan        nan\n",
      " 0.5               nan 0.5        0.84112782 0.84112782 0.84218881\n",
      " 0.84218881 0.84218881        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.86174882 0.86174882 0.85964355 0.85964355 0.85964355\n",
      "        nan        nan 0.81208299        nan 0.81208299 0.88811473\n",
      " 0.88811473 0.88599276 0.88704818 0.88493734        nan        nan\n",
      " 0.8654386         nan 0.86385408 0.89019215 0.89019215 0.88864383\n",
      " 0.89180451 0.88599833        nan        nan 0.88915065        nan\n",
      " 0.88758563 0.88753551 0.88753551 0.89232804 0.89022556 0.88546923\n",
      "        nan        nan 0.88595656        nan 0.88494291 0.88542746\n",
      " 0.88489836 0.89176831 0.88917293 0.88494291        nan        nan\n",
      " 0.88436926        nan 0.88546923 0.88436926 0.88436926 0.88595377\n",
      " 0.8886494  0.88546923        nan        nan 0.88436926        nan\n",
      " 0.88494291 0.88436926 0.88436926 0.88436926 0.88864662 0.88546923]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.81996658\n",
      " 0.81996658 0.81997215 0.81997215 0.81997215        nan        nan\n",
      " 0.5               nan 0.5        0.85328599 0.85328599 0.85381509\n",
      " 0.85381509 0.85381509        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.88443331 0.88443331 0.88392091 0.88392091 0.88392091\n",
      "        nan        nan 0.81950153        nan 0.81897243 0.90602896\n",
      " 0.90602896 0.91183793 0.91236703 0.91236703        nan        nan\n",
      " 0.89128098        nan 0.89021442 0.89387914 0.89387914 0.90918129\n",
      " 0.90918964 0.91025063        nan        nan 0.90757728        nan\n",
      " 0.90919243 0.89598719 0.89598719 0.90811752 0.9112949  0.90866889\n",
      "        nan        nan 0.90810916        nan 0.91024784 0.89494013\n",
      " 0.89494013 0.90705931 0.91129212 0.90866889        nan        nan\n",
      " 0.90863548        nan 0.90866889 0.89546644 0.89546644 0.90811473\n",
      " 0.91129212 0.90866889        nan        nan 0.90547201        nan\n",
      " 0.90866889 0.89441381 0.89441381 0.91022278 0.91023949 0.90866889]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.82049568\n",
      " 0.82049568 0.81944305 0.81944305 0.81944305        nan        nan\n",
      " 0.5               nan 0.5        0.85327764 0.85327764 0.85169312\n",
      " 0.85169312 0.85221944        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.88708716 0.88708716 0.88657477 0.88657199 0.88657199\n",
      "        nan        nan 0.80573656        nan 0.80573656 0.89076859\n",
      " 0.89076859 0.8912949  0.89288221 0.89235589        nan        nan\n",
      " 0.88495127        nan 0.88336118 0.88338067 0.88338067 0.87863548\n",
      " 0.8823169  0.88600111        nan        nan 0.87018379        nan\n",
      " 0.88653578 0.88970482 0.88970482 0.87758563 0.88336953 0.88652743\n",
      "        nan        nan 0.86491785        nan 0.88652743 0.88917572\n",
      " 0.88917572 0.87652186 0.88284043 0.88758563        nan        nan\n",
      " 0.87229184        nan 0.88705653 0.88390699 0.88390699 0.87494013\n",
      " 0.88389585 0.88758563        nan        nan 0.88019493        nan\n",
      " 0.88705653 0.88180173 0.88127541 0.87441103 0.88336953 0.88652743]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.81890281\n",
      " 0.81890281 0.8189056  0.8189056  0.8189056         nan        nan\n",
      " 0.5               nan 0.5        0.84904205 0.84904205 0.85168198\n",
      " 0.85168198 0.85221108        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.89235867 0.89235867 0.89500696 0.89500696 0.89500696\n",
      "        nan        nan 0.80838207        nan 0.80785297 0.90976608\n",
      " 0.90976608 0.91242551 0.91347536 0.91347536        nan        nan\n",
      " 0.90240602        nan 0.90292398 0.90449735 0.90449735 0.89289613\n",
      " 0.90081314 0.90767196        nan        nan 0.89920356        nan\n",
      " 0.90872737 0.89500696 0.89500696 0.8807296  0.90344472 0.90767474\n",
      "        nan        nan 0.89443052        nan 0.90767474 0.89394598\n",
      " 0.89394598 0.8807296  0.90239209 0.90767474        nan        nan\n",
      " 0.88492063        nan 0.90767474 0.891824   0.891824   0.88019215\n",
      " 0.90238931 0.90767474        nan        nan 0.86219716        nan\n",
      " 0.90767474 0.8907658  0.88971317 0.87966583 0.90239209 0.90767474]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.8183765\n",
      " 0.8183765  0.8183765  0.8183765  0.8183765         nan        nan\n",
      " 0.5               nan 0.5        0.85062935 0.85062935 0.85116124\n",
      " 0.85116124 0.85168755        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.88390977 0.88390977 0.88813422 0.88866332 0.88866332\n",
      "        nan        nan 0.80785297        nan 0.80679476 0.90397661\n",
      " 0.90397661 0.90399053 0.90557226 0.90451685        nan        nan\n",
      " 0.87653857        nan 0.88234754 0.89500139 0.89500139 0.88972988\n",
      " 0.89079087 0.89976608        nan        nan 0.87811473        nan\n",
      " 0.90188527 0.88971596 0.88971596 0.88340295 0.89184907 0.89924255\n",
      "        nan        nan 0.87496519        nan 0.89924534 0.88602896\n",
      " 0.88550265 0.87759955 0.89184907 0.89924255        nan        nan\n",
      " 0.87018379        nan 0.89871345 0.88549986 0.88602896 0.87654135\n",
      " 0.89290448 0.89765803        nan        nan 0.85009468        nan\n",
      " 0.89765803 0.88338903 0.88550265 0.87179337 0.89237817 0.89765803]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.82048454\n",
      " 0.82048454 0.81995823 0.81995823 0.81995823        nan        nan\n",
      " 0.5               nan 0.5        0.8511501  0.8511501  0.85590922\n",
      " 0.85538012 0.85538012        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.8955472  0.8955472  0.90029518 0.89976887 0.89976887\n",
      "        nan        nan 0.80679476        nan 0.80679476 0.90927597\n",
      " 0.90927597 0.91246449 0.91246449 0.91141186        nan        nan\n",
      " 0.88976051        nan 0.89451406 0.89818435 0.89818435 0.89715957\n",
      " 0.90138123 0.90612921        nan        nan 0.89026455        nan\n",
      " 0.90770816 0.89292398 0.89292398 0.88818435 0.90243665 0.90665553\n",
      "        nan        nan 0.88184071        nan 0.90612643 0.88659705\n",
      " 0.88659705 0.88343637 0.90296575 0.90665553        nan        nan\n",
      " 0.86176831        nan 0.90665553 0.88396547 0.8850181  0.87869674\n",
      " 0.90296575 0.90665553        nan        nan 0.83483988        nan\n",
      " 0.90665553 0.88344194 0.88344194 0.87975494 0.90349485 0.90612643]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.81784461\n",
      " 0.81784461 0.8178474  0.8178474  0.8178474         nan        nan\n",
      " 0.5               nan 0.5        0.8516792  0.8516792  0.85326371\n",
      " 0.85326371 0.85326371        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.89290727 0.89290727 0.8981927  0.8971345  0.8971345\n",
      "        nan        nan 0.80732387        nan 0.80679476 0.91352827\n",
      " 0.91352827 0.91564467 0.91564467 0.91617098        nan        nan\n",
      " 0.87658034        nan 0.88028126 0.90666388 0.90666388 0.90878307\n",
      " 0.91406572 0.91406015        nan        nan 0.88236981        nan\n",
      " 0.91299916 0.89928989 0.89928989 0.89875522 0.91406572 0.91353105\n",
      "        nan        nan 0.87289056        nan 0.91353105 0.89453356\n",
      " 0.89453356 0.89612086 0.91406572 0.91406015        nan        nan\n",
      " 0.86281537        nan 0.91353105 0.89137009 0.89084378 0.88769702\n",
      " 0.91353383 0.91406015        nan        nan 0.82426901        nan\n",
      " 0.91353105 0.88662211 0.89401281 0.88769424 0.91406294 0.91353105]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.81731551\n",
      " 0.81731551 0.81784461 0.81784461 0.81784461        nan        nan\n",
      " 0.5               nan 0.5        0.85221108 0.85221108 0.85274018\n",
      " 0.85274018 0.85274018        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.89186578 0.89186578 0.89451128 0.89398218 0.89398218\n",
      "        nan        nan 0.80679476        nan 0.80679476 0.91616541\n",
      " 0.91616541 0.91142022 0.9108939  0.91194653        nan        nan\n",
      " 0.88240323        nan 0.88556391 0.90667502 0.90667502 0.90455026\n",
      " 0.90350599 0.91036202        nan        nan 0.88819827        nan\n",
      " 0.9093066  0.90719577 0.90719577 0.90297132 0.90350599 0.9098357\n",
      "        nan        nan 0.87604288        nan 0.9087775  0.90666945\n",
      " 0.90666945 0.90457254 0.90350599 0.9098357         nan        nan\n",
      " 0.84537455        nan 0.9098357  0.90244222 0.90244222 0.89615706\n",
      " 0.9040323  0.9108939         nan        nan 0.82526873        nan\n",
      " 0.9103648  0.902445   0.90562796 0.89247006 0.90297689 0.91036202]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.81783904\n",
      " 0.81783904 0.81784183 0.81784183 0.81784183        nan        nan\n",
      " 0.5               nan 0.5        0.8516792  0.8516792  0.85485102\n",
      " 0.85485102 0.8543247         nan        nan 0.78772765        nan\n",
      " 0.78772765 0.89608187 0.89608187 0.89873851 0.89873851 0.89873851\n",
      "        nan        nan 0.80679476        nan 0.80679476 0.91351991\n",
      " 0.91351991 0.91247006 0.91246728 0.91299638        nan        nan\n",
      " 0.87817878        nan 0.88663047 0.90823726 0.90823726 0.90348649\n",
      " 0.90665274 0.90982735        nan        nan 0.86019493        nan\n",
      " 0.90455305 0.90612086 0.90612086 0.89978279 0.90665274 0.90982456\n",
      "        nan        nan 0.86178502        nan 0.90876915 0.90875522\n",
      " 0.90875522 0.89926483 0.90612364 0.90929546        nan        nan\n",
      " 0.85696463        nan 0.90929546 0.90770259 0.90770259 0.89344751\n",
      " 0.90612364 0.90982456        nan        nan 0.80358674        nan\n",
      " 0.90982456 0.90454191 0.90294625 0.89450014 0.90717906 0.90876636]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.81837093\n",
      " 0.81837093 0.8173183  0.8173183  0.8173183         nan        nan\n",
      " 0.5               nan 0.5        0.85168755 0.85168755 0.85380117\n",
      " 0.85380117 0.85380117        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.88816486 0.88816486 0.88974659 0.88974659 0.88922027\n",
      "        nan        nan 0.80679476        nan 0.80679476 0.89449457\n",
      " 0.89502367 0.89713729 0.89713729 0.8971345         nan        nan\n",
      " 0.86601504        nan 0.86865776 0.88761348 0.88761348 0.88867446\n",
      " 0.89079087 0.89132554        nan        nan 0.85228627        nan\n",
      " 0.89186299 0.8828655  0.8828655  0.88392927 0.89079087 0.89185185\n",
      "        nan        nan 0.84858535        nan 0.89185185 0.88549986\n",
      " 0.88549986 0.87126706 0.89237538 0.89132554        nan        nan\n",
      " 0.81783904        nan 0.89132554 0.88338903 0.88444723 0.86493177\n",
      " 0.89184907 0.89185185        nan        nan 0.77718184        nan\n",
      " 0.89079644 0.88076023 0.88234754 0.86123642 0.89131997 0.89026734]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.81836814\n",
      " 0.81836814 0.81784461 0.81784461 0.81784461        nan        nan\n",
      " 0.5               nan 0.5        0.8522083  0.8522083  0.85432749\n",
      " 0.85432749 0.85379838        nan        nan 0.78772765        nan\n",
      " 0.78772765 0.88499025 0.88499025 0.88711779 0.88711779 0.88711779\n",
      "        nan        nan 0.80679476        nan 0.80679476 0.89238931\n",
      " 0.89238931 0.89239488 0.89239488 0.89239209        nan        nan\n",
      " 0.8591061         nan 0.87549986 0.88552214 0.88552214 0.8897633\n",
      " 0.89028683 0.89345586        nan        nan 0.86018658        nan\n",
      " 0.89028683 0.88235032 0.88235032 0.88553885 0.89028126 0.89292398\n",
      "        nan        nan 0.85120301        nan 0.89397939 0.87971039\n",
      " 0.87971039 0.87602896 0.88975494 0.89292676        nan        nan\n",
      " 0.83211083        nan 0.89292398 0.87812865 0.87760234 0.86018936\n",
      " 0.89028126 0.89239766        nan        nan 0.73969925        nan\n",
      " 0.89292398 0.87549708 0.87655528 0.85753829 0.88975216 0.89345308]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_components:\n",
      "110\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################random seed loop#################\n",
    "train_scores=[]\n",
    "test_scores=[]\n",
    "loopn=1\n",
    "scoring='roc_auc'\n",
    "#np.random.seed(42)\n",
    "random_states=np.random.choice(range(101), loopn, replace=False)\n",
    "\n",
    "tmp_train=[]\n",
    "tmp_test=[]\n",
    "#print(random_states)\n",
    "for i in range(loopn):\n",
    "    train_X,test_X, train_y, test_y = train_test_split(train,\n",
    "                                                   target,\n",
    "                                                   test_size = 0.3,\n",
    "                                                   stratify=target,\n",
    "                                                   random_state=10\n",
    "                                                   )\n",
    "    print(\"train_x.shpae:\")\n",
    "    print(train_X.shape)\n",
    "\n",
    "    standardScaler = StandardScaler()\n",
    "    standardScaler.fit(train_X)\n",
    "    X_standard = standardScaler.transform(train_X)\n",
    "    X_standard_test = standardScaler.transform(test_X)\n",
    "    #calculate max n_components\n",
    "    estimator = PCA(n_components=0.95,random_state=10)\n",
    "    pca_X_train = estimator.fit_transform(X_standard)\n",
    "\n",
    "    n_components=range(10,min(pca_X_train.shape),10)\n",
    "    print(n_components)\n",
    "    #n_components=[0.99,0.95,0.90,0.85]\n",
    "    best_pca_train_scores=[]\n",
    "    best_pca_test_scores=[]\n",
    "    for j in n_components:\n",
    "        estimator = PCA(n_components=j,random_state=10)\n",
    "        pca_X_train = estimator.fit_transform(X_standard)\n",
    "        pca_X_test = estimator.transform(X_standard_test)\n",
    "        cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=10) \n",
    "    \n",
    "        #scoring = {'AUC': 'roc_auc'}\n",
    "        cost = [-5, -3, -1, 1, 3, 5, 7, 9, 11, 13, 15]\n",
    "        gam = [3, 1, -1, -3, -5, -7, -9, -11, -13, -15]\n",
    "        parameters =[{'kernel': ['rbf'], 'C': [2**x for x in cost],'gamma':[2**x for x in gam]}]\n",
    "\n",
    "        svc_grid_search=GridSearchCV(estimator=SVC(random_state=10),\n",
    "                                     param_grid=parameters,cv=cvx,scoring=scoring)\n",
    "        svc_grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "        param_grid = {'penalty':['l1', 'l2'],\n",
    "                      \"C\":[0.00001,0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                      \"solver\":[\"newton-cg\", \"lbfgs\",\"liblinear\",\"sag\",\"saga\"]\n",
    "    #               \"algorithm\":['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "                    }\n",
    "        LR_grid = LogisticRegression(max_iter=1000, random_state=10)\n",
    "        LR_grid_search = GridSearchCV(LR_grid, param_grid=param_grid, cv=cvx ,scoring=scoring,n_jobs=10)\n",
    "        LR_grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "        estimators = [\n",
    "            ('lr', LR_grid_search.best_estimator_),\n",
    "            ('svc', svc_grid_search.best_estimator_),\n",
    "        ]\n",
    "        clf = StackingClassifier(estimators=estimators, final_estimator=LinearSVC(C=5, random_state=10),\n",
    "                                 n_jobs=10)\n",
    "        clf.fit(pca_X_train, train_y)\n",
    "\n",
    "        estimators = [\n",
    "            ('lr', LR_grid_search.best_estimator_),\n",
    "            ('svc', svc_grid_search.best_estimator_),\n",
    "        ]\n",
    "\n",
    "        param_grid = {'final_estimator':[LogisticRegression(C=0.00001),LogisticRegression(C=0.0001),\n",
    "                                         LogisticRegression(C=0.001),LogisticRegression(C=0.01),\n",
    "                                         LogisticRegression(C=0.1),LogisticRegression(C=1),\n",
    "                                         LogisticRegression(C=10),LogisticRegression(C=100),\n",
    "                                         LogisticRegression(C=1000)]}\n",
    "\n",
    "        Stacking_grid =StackingClassifier(estimators=estimators,)\n",
    "        Stacking_grid_search = GridSearchCV(Stacking_grid, param_grid=param_grid, cv=cvx,\n",
    "                                            scoring=scoring,n_jobs=10)\n",
    "        Stacking_grid_search.fit(pca_X_train, train_y)\n",
    "        Stacking_grid_search.best_estimator_\n",
    "\n",
    "        train_pre_y = cross_val_predict(Stacking_grid_search.best_estimator_, pca_X_train,train_y, cv=cvx)\n",
    "        train_res1=get_measures_gridloo(train_y,train_pre_y)\n",
    "\n",
    "        test_pre_y = Stacking_grid_search.predict(pca_X_test)\n",
    "        test_res1=get_measures_gridloo(test_y,test_pre_y)\n",
    "        \n",
    "        best_pca_train_scores.append(train_res1.loc[:,\"AUC\"])\n",
    "        best_pca_test_scores.append(test_res1.loc[:,\"AUC\"])\n",
    "    \n",
    "    train_scores.append(np.max(best_pca_train_scores))\n",
    "    test_scores.append(best_pca_test_scores[np.argmax(best_pca_train_scores)])\n",
    "    print(\"n_components:\")\n",
    "    print(n_components[np.argmax(best_pca_train_scores)])\n",
    "    \n",
    "    tmp_train.append(best_pca_train_scores)\n",
    "    tmp_test.append(best_pca_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e38686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "0.8376952300548755\n",
      "0.7829059829059828\n",
      "std\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0    0.754221\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.809941\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.805087\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.827406\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.797119\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.811102\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.795958\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.805667\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.826826\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.833421\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.837695\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.817697\n",
       "Name: AUC, dtype: float64</td>\n",
       "      <td>0    0.822552\n",
       "Name: AUC, dtype: float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0   \\\n",
       "0  0    0.754221\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        1   \\\n",
       "0  0    0.809941\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        2   \\\n",
       "0  0    0.805087\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        3   \\\n",
       "0  0    0.827406\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        4   \\\n",
       "0  0    0.797119\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        5   \\\n",
       "0  0    0.811102\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        6   \\\n",
       "0  0    0.795958\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        7   \\\n",
       "0  0    0.805667\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        8   \\\n",
       "0  0    0.826826\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        9   \\\n",
       "0  0    0.833421\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        10  \\\n",
       "0  0    0.837695\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        11  \\\n",
       "0  0    0.817697\n",
       "Name: AUC, dtype: float64   \n",
       "\n",
       "                                        12  \n",
       "0  0    0.822552\n",
       "Name: AUC, dtype: float64  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('mean')\n",
    "print(np.mean(train_scores))\n",
    "print(np.mean(test_scores))\n",
    "print(\"std\")\n",
    "print(np.std(train_scores))\n",
    "print(np.std(test_scores))\n",
    "pd.DataFrame(tmp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "03767256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9295183982683983]\n",
      "[0    0.940972\n",
      "Name: AUC, dtype: float64]\n"
     ]
    }
   ],
   "source": [
    "print(train_scores)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360d174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c96d22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shpae:\n",
      "(643, 4660)\n",
      "range(10, 225, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.7143901\n",
      " 0.7143901  0.71497091 0.71497091 0.71497091        nan        nan\n",
      " 0.5               nan 0.5        0.71448764 0.71448764 0.71269409\n",
      " 0.71288869 0.71283985        nan        nan 0.75544363        nan\n",
      " 0.75544363 0.7410049  0.7410049  0.71928803 0.72452302 0.72074056\n",
      "        nan        nan 0.72603681        nan 0.72980901 0.77485496\n",
      " 0.77485496 0.73512124 0.72803268 0.72116555        nan        nan\n",
      " 0.76355163        nan 0.72089623 0.78226702 0.78226702 0.76229474\n",
      " 0.72811836 0.72164791        nan        nan 0.78163126        nan\n",
      " 0.72198905 0.78337635 0.78337635 0.78181982 0.7286038  0.72184327\n",
      "        nan        nan 0.78313284        nan 0.72184327 0.78327786\n",
      " 0.78322902 0.78332596 0.72865188 0.72184327        nan        nan\n",
      " 0.78318092        nan 0.72184327 0.78327709 0.78327709 0.78322899\n",
      " 0.72865188 0.72184327        nan        nan 0.78327709        nan\n",
      " 0.72184327 0.78327709 0.78327709 0.78327709 0.72865188 0.72184327]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.72404662\n",
      " 0.72404662 0.72438469 0.72438393 0.724432          nan        nan\n",
      " 0.5               nan 0.5        0.72693857 0.72693857 0.72466103\n",
      " 0.72480607 0.72475726        nan        nan 0.75544363        nan\n",
      " 0.75544363 0.75539916 0.75539916 0.73552226 0.73877114 0.7357191\n",
      "        nan        nan 0.73102541        nan 0.73525017 0.79135531\n",
      " 0.79135531 0.7484571  0.7410591  0.74123069        nan        nan\n",
      " 0.78985385        nan 0.73708003 0.79798246 0.79798246 0.78495366\n",
      " 0.74095307 0.73972867        nan        nan 0.79767193        nan\n",
      " 0.73992621 0.79815788 0.79815788 0.79650518 0.74042259 0.73924171\n",
      "        nan        nan 0.79771754        nan 0.7393882  0.79776786\n",
      " 0.79776786 0.79776793 0.74051876 0.73914477        nan        nan\n",
      " 0.79771976        nan 0.73919287 0.79771976 0.79767168 0.79776712\n",
      " 0.74051876 0.73914477        nan        nan 0.79771976        nan\n",
      " 0.73914477 0.79771976 0.79771976 0.79771976 0.74051876 0.73914477]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.72331987\n",
      " 0.72331987 0.72336945 0.72336945 0.72336943        nan        nan\n",
      " 0.5               nan 0.5        0.72544599 0.72544599 0.72283712\n",
      " 0.72283564 0.7227868         nan        nan 0.75544363        nan\n",
      " 0.75544363 0.74926883 0.74926883 0.73458148 0.73666923 0.73477971\n",
      "        nan        nan 0.73204358        nan 0.73674485 0.77676985\n",
      " 0.77686753 0.74080672 0.73961927 0.7353765         nan        nan\n",
      " 0.77672052        nan 0.73215426 0.78521532 0.78521532 0.76502745\n",
      " 0.73522105 0.73561325        nan        nan 0.78476295        nan\n",
      " 0.73532761 0.785009   0.7849609  0.78224271 0.73507523 0.73551703\n",
      "        nan        nan 0.78534489        nan 0.73561395 0.78476924\n",
      " 0.78476924 0.7842846  0.73502715 0.73551703        nan        nan\n",
      " 0.7848166         nan 0.73551703 0.7846723  0.7846723  0.78452504\n",
      " 0.73502715 0.73551703        nan        nan 0.78467156        nan\n",
      " 0.73551703 0.7846723  0.7846723  0.78462346 0.73502715 0.73546819]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.72794032\n",
      " 0.72794032 0.72828135 0.72823325 0.7283775         nan        nan\n",
      " 0.5               nan 0.5        0.73331264 0.7332638  0.73151527\n",
      " 0.73141914 0.73127262        nan        nan 0.75544363        nan\n",
      " 0.75544363 0.76566651 0.76566651 0.74847951 0.75181719 0.74925741\n",
      "        nan        nan 0.73247565        nan 0.73722803 0.80265776\n",
      " 0.80265778 0.75127794 0.74973147 0.74976898        nan        nan\n",
      " 0.79599819        nan 0.74334131 0.82788915 0.82774487 0.7853092\n",
      " 0.7446121  0.74908737        nan        nan 0.82797649        nan\n",
      " 0.74899129 0.83063291 0.83072985 0.82097231 0.74441447 0.7488943\n",
      "        nan        nan 0.8304863         nan 0.74898971 0.83072742\n",
      " 0.8307755  0.82975675 0.74441447 0.7488943         nan        nan\n",
      " 0.83077554        nan 0.74894238 0.83077554 0.83082438 0.83072666\n",
      " 0.74441447 0.7488943         nan        nan 0.83077554        nan\n",
      " 0.7488943  0.83077554 0.83077554 0.83077554 0.74441447 0.7488943 ]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "train_X,test_X, train_y, test_y = train_test_split(train,\n",
    "                                                   target,\n",
    "                                                   test_size = 0.3,\n",
    "                                                   stratify=target,\n",
    "                                                   random_state=10\n",
    "                                                   )\n",
    "print(\"train_x.shpae:\")\n",
    "print(train_X.shape)\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "standardScaler.fit(train_X)\n",
    "X_standard = standardScaler.transform(train_X)\n",
    "X_standard_test = standardScaler.transform(test_X)\n",
    "    #calculate max n_components\n",
    "estimator = PCA(n_components=0.95,random_state=10)\n",
    "pca_X_train = estimator.fit_transform(X_standard)\n",
    "\n",
    "n_components=range(10,min(pca_X_train.shape),10)\n",
    "print(n_components)\n",
    "    #n_components=[0.99,0.95,0.90,0.85]\n",
    "best_pca_train_scores=[]\n",
    "best_pca_test_scores=[]\n",
    "for j in n_components:\n",
    "    estimator = PCA(n_components=j,random_state=10)\n",
    "    pca_X_train = estimator.fit_transform(X_standard)\n",
    "    pca_X_test = estimator.transform(X_standard_test)\n",
    "    cvx = StratifiedKFold(n_splits=5, shuffle=True, random_state=10) \n",
    "    \n",
    "        #scoring = {'AUC': 'roc_auc'}\n",
    "    cost = [-5, -3, -1, 1, 3, 5, 7, 9, 11, 13, 15]\n",
    "    gam = [3, 1, -1, -3, -5, -7, -9, -11, -13, -15]\n",
    "    parameters =[{'kernel': ['rbf'], 'C': [2**x for x in cost],'gamma':[2**x for x in gam]}]\n",
    "\n",
    "    svc_grid_search=GridSearchCV(estimator=SVC(random_state=10),\n",
    "                                 param_grid=parameters,cv=cvx,scoring=scoring)\n",
    "    svc_grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "    param_grid = {'penalty':['l1', 'l2'],\n",
    "                  \"C\":[0.00001,0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                  \"solver\":[\"newton-cg\", \"lbfgs\",\"liblinear\",\"sag\",\"saga\"]\n",
    "    #               \"algorithm\":['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "                  }\n",
    "    LR_grid = LogisticRegression(max_iter=1000, random_state=10)\n",
    "    LR_grid_search = GridSearchCV(LR_grid, param_grid=param_grid, cv=cvx ,scoring=scoring,n_jobs=10)\n",
    "    LR_grid_search.fit(pca_X_train, train_y)\n",
    "\n",
    "    estimators = [\n",
    "        ('lr', LR_grid_search.best_estimator_),\n",
    "        ('svc', svc_grid_search.best_estimator_),\n",
    "    ]\n",
    "    clf = StackingClassifier(estimators=estimators, final_estimator=LinearSVC(C=5, random_state=10),\n",
    "                             n_jobs=10)\n",
    "    clf.fit(pca_X_train, train_y)\n",
    "\n",
    "    estimators = [\n",
    "        ('lr', LR_grid_search.best_estimator_),\n",
    "        ('svc', svc_grid_search.best_estimator_),\n",
    "    ]\n",
    "\n",
    "    param_grid = {'final_estimator':[LogisticRegression(C=0.00001),LogisticRegression(C=0.0001),\n",
    "                                    LogisticRegression(C=0.001),LogisticRegression(C=0.01),\n",
    "                                    LogisticRegression(C=0.1),LogisticRegression(C=1),\n",
    "                                    LogisticRegression(C=10),LogisticRegression(C=100),\n",
    "                                    LogisticRegression(C=1000)]}\n",
    "\n",
    "    Stacking_grid =StackingClassifier(estimators=estimators,)\n",
    "    Stacking_grid_search = GridSearchCV(Stacking_grid, param_grid=param_grid, cv=cvx,\n",
    "                                        scoring=scoring,n_jobs=10)\n",
    "    Stacking_grid_search.fit(pca_X_train, train_y)\n",
    "    Stacking_grid_search.best_estimator_\n",
    "\n",
    "    train_pre_y = cross_val_predict(Stacking_grid_search.best_estimator_, pca_X_train,train_y, cv=cvx)\n",
    "    train_res1=get_measures_gridloo(train_y,train_pre_y)\n",
    "\n",
    "    test_pre_y = Stacking_grid_search.predict(pca_X_test)\n",
    "    test_res1=get_measures_gridloo(test_y,test_pre_y)\n",
    "        \n",
    "    best_pca_train_scores.append(train_res1.loc[:,\"AUC\"])\n",
    "    best_pca_test_scores.append(test_res1.loc[:,\"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2066b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7679131343630241\n",
      "0    0.731618\n",
      "Name: AUC, dtype: float64\n",
      "n_components:\n",
      "140\n",
      "[0    0.735377\n",
      "Name: AUC, dtype: float64, 0    0.718025\n",
      "Name: AUC, dtype: float64, 0    0.745921\n",
      "Name: AUC, dtype: float64, 0    0.750508\n",
      "Name: AUC, dtype: float64, 0    0.747663\n",
      "Name: AUC, dtype: float64, 0    0.761797\n",
      "Name: AUC, dtype: float64, 0    0.750668\n",
      "Name: AUC, dtype: float64, 0    0.763326\n",
      "Name: AUC, dtype: float64, 0    0.756944\n",
      "Name: AUC, dtype: float64, 0    0.746134\n",
      "Name: AUC, dtype: float64, 0    0.749405\n",
      "Name: AUC, dtype: float64, 0    0.757103\n",
      "Name: AUC, dtype: float64, 0    0.744552\n",
      "Name: AUC, dtype: float64, 0    0.767913\n",
      "Name: AUC, dtype: float64, 0    0.739911\n",
      "Name: AUC, dtype: float64, 0    0.732213\n",
      "Name: AUC, dtype: float64, 0    0.730524\n",
      "Name: AUC, dtype: float64, 0    0.726043\n",
      "Name: AUC, dtype: float64, 0    0.729155\n",
      "Name: AUC, dtype: float64, 0    0.732053\n",
      "Name: AUC, dtype: float64, 0    0.732266\n",
      "Name: AUC, dtype: float64, 0    0.73543\n",
      "Name: AUC, dtype: float64]\n",
      "[0    0.739706\n",
      "Name: AUC, dtype: float64, 0    0.724895\n",
      "Name: AUC, dtype: float64, 0    0.703361\n",
      "Name: AUC, dtype: float64, 0    0.735714\n",
      "Name: AUC, dtype: float64, 0    0.735819\n",
      "Name: AUC, dtype: float64, 0    0.728466\n",
      "Name: AUC, dtype: float64, 0    0.728256\n",
      "Name: AUC, dtype: float64, 0    0.739181\n",
      "Name: AUC, dtype: float64, 0    0.743277\n",
      "Name: AUC, dtype: float64, 0    0.739076\n",
      "Name: AUC, dtype: float64, 0    0.724685\n",
      "Name: AUC, dtype: float64, 0    0.717542\n",
      "Name: AUC, dtype: float64, 0    0.732038\n",
      "Name: AUC, dtype: float64, 0    0.731618\n",
      "Name: AUC, dtype: float64, 0    0.720903\n",
      "Name: AUC, dtype: float64, 0    0.709769\n",
      "Name: AUC, dtype: float64, 0    0.702416\n",
      "Name: AUC, dtype: float64, 0    0.695273\n",
      "Name: AUC, dtype: float64, 0    0.702521\n",
      "Name: AUC, dtype: float64, 0    0.698634\n",
      "Name: AUC, dtype: float64, 0    0.680567\n",
      "Name: AUC, dtype: float64, 0    0.680672\n",
      "Name: AUC, dtype: float64]\n"
     ]
    }
   ],
   "source": [
    "print(np.max(best_pca_train_scores))\n",
    "print(best_pca_test_scores[np.argmax(best_pca_train_scores)])\n",
    "print(\"n_components:\")\n",
    "print(n_components[np.argmax(best_pca_train_scores)])\n",
    "\n",
    "print(best_pca_train_scores)\n",
    "print(best_pca_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c013e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
